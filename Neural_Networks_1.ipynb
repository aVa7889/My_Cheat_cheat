{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the X and y\n",
    "X = df.drop(columns=\"Target\")\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Use sklearn to split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler instance\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "X_scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Keras Sequential model\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Set input nodes to the number of features\n",
    "input_nodes = len(X.columns)\n",
    "\n",
    "# Add our first Dense layer, including the input layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"relu\", input_dim=input_nodes))\n",
    "# Add the output layer that uses a probability activation function\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "# Check the structure of the Sequential model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model to the training data\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"  \")\n",
    "df.plot.scatter(x=\"Feature 1\", y=\"Feature 2\", c=\"Target\", colormap='winter')\n",
    "\n",
    "X = df.drop(columns=[\"Target\"], axis=1)\n",
    "y = df[\"Target\"]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "\n",
    "X_scaler = skl.preprocessing.StandardScaler()\n",
    "X_Scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model2 = tf.keras.models.Sequential()\n",
    "nn_model2.add(tf.keras.layers.Dense(units-6, activation=\"relu\", input_dim=input_nodes))\n",
    "nn_model2.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "nn_model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "fit_model = nn_model2.fit(X_train_scaled, y_train, epochs=100)\n",
    "model_loss, model_accuracy = nn_model2.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restricted Boltzmann Machines (RBMs) are a type of neural network used in deep learning for unsupervised learning tasks. Here are some scenarios when RBMs are typically used:\n",
    "\n",
    "# Dimensionality Reduction: RBMs can reduce the number of features in a dataset while preserving important information, making it easier to visualize or further process [4].\n",
    "\n",
    "# Feature Learning: RBMs can learn complex patterns and features from the input data, which can then be used as inputs for other models, such as classification or regression models [4].\n",
    "\n",
    "# Collaborative Filtering: They are used in recommendation systems, like Netflix's movie recommendation engine, to predict user preferences based on learned patterns [4].\n",
    "\n",
    "# Data Reconstruction: RBMs can reconstruct input data from their hidden layer, making them useful for tasks like image denoising [3].\n",
    "\n",
    "# Pre-training Deep Networks: RBMs can be used to pre-train deep neural networks layer-by-layer, initializing the network weights in a way that can lead to better performance and faster convergence [2].\n",
    "\n",
    "# These applications make RBMs a valuable tool in scenarios where discovering hidden patterns or reducing complexity in data is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network models can train on raw numerical data, but training them on raw \n",
    "#data often is not a good idea"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
