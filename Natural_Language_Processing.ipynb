{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "# Natural Language Toolkit (NLTK) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import reuters database from the nltk corpus \n",
    "from nltk.corpus import reuters, stopwords\n",
    "# Import tokenizers\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.util import ngrams\n",
    "# Import the WordNetLemmatizer class \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Download the reuters corpora and the \"punkt\" sentence tokenizer.\n",
    "import nltk\n",
    "nltk.download(\"reuters\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import re\n",
    "# Import the Counter class from the collections library.\n",
    "from collections import Counter\n",
    "\n",
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the nltk stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "print(sw)\n",
    "\n",
    "# Filter out all the stopwords from the words in the sentence.\n",
    "first_result = [word.lower() for word in all_words if word.lower() not in sw]\n",
    "print(first_result)\n",
    "\n",
    "# We can define our own list of stopwords to add to the default nltk stopwords\n",
    "sw_addon = {'still', 'fifty-four'}\n",
    "second_result = [word.lower() for word in all_words if word.lower() not in sw.union(sw_addon)]\n",
    "print(second_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substitute everything that is not a letter with an empty string\n",
    "regex = re.compile(\"[^a-zA-Z ]\")\n",
    "re_clean = regex.sub(' ', one_sentence)\n",
    "print(re_clean)\n",
    "\n",
    "# Retrieve everything that is not a letter with an empty string\n",
    "re_clean_2 = re.findall(\"[^a-zA-Z ]\", one_sentence)\n",
    "print(re_clean_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Get the categories\n",
    "print(reuters.categories())\n",
    "cpi_article = \"\"\"AVA LEE \n",
    "\n",
    "PROFILE SUMMARY\n",
    "\"\"\"\n",
    "def process_text(article):\n",
    "    # Get the stopwords\n",
    "    sw = set(stopwords.words('english'))\n",
    "    # Use regex to substitute everything that is not a letter with an empty string.\n",
    "    regex = re.compile(\"[^a-zA-Z]\")\n",
    "    re_clean = regex.sub('', article)\n",
    "    # Tokenize the words \n",
    "    words = word_tokenize(re_clean)\n",
    "    # Lemmatize the words\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Retrieve only the words that aren't in the stopwords\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return output\n",
    "\n",
    "# Pass the article to function and print the processed text.\n",
    "processed_article = process_text(cpi_article)\n",
    "print(processed_article)\n",
    "\n",
    "# Get the word counts by passing in the processed article to the Counter class.\n",
    "word_counts = Counter(processed_article)\n",
    "# Print the dictionary of the word counts.\n",
    "print(dict(word_counts))\n",
    "\n",
    "# Print the top 10 most common words.\n",
    "print(dict(word_counts.most_common(10)))\n",
    "\n",
    "# Get the number of bigrams.\n",
    "bigram_counts = Counter(ngrams(processed_article, n=2))\n",
    "print(dict(bigram_counts))\n",
    "\n",
    "# Print the top 5 most common bigrams\n",
    "print(dict(bigram_counts.most_common(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
